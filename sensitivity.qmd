# Sensitivity analysis {#sec-sensitivity}

Sensitivity analysis broadly refers to assessing the sensitivity of results to violations of some of the assumptions made. In propensity score analysis, it usually refers to sensitivity to the assumption of satisfaction of the backdoor criterion, i.e., no unmeasured confounding. There is no simple way to test whether observed results are confounded or not; instead, we assess how much our estimates would change for different degrees of hypothetical confounding and, in particular, how strong confounding would have to be to change our inferences (i.e., on the presence or direction of the effect). Sensitivity analysis is an area of ongoing research; presently, there is no single best method to use or one that has achieved widespread adoption.

@dagostinomcgowanSensitivityAnalysesUnmeasured2022 provide a review of sensitivity analysis methods along with a description of software that implements many of them. Perhaps the most common method of assessing sensitivity to unmeasured confounding is a single-number summary called the "E-value" [@vanderweeleSensitivityAnalysisObservational2017; @haneuseUsingEValueAssess2019], though it is also one of the most controversial and misinterpreted [@ioannidisLimitationsMisinterpretationsEValues2019; @vanderweeleCorrectingMisinterpretationsEValue2019]. The E-value is a transformation of the effect estimate that corresponds to the minimum strength of association (on the risk ratio scale) that a hypothetical unmeasured confounder would have to have with the treatment and outcome to fully explain away an observed effect estimate.

Because there are no agreed-upon standards for reporting sensitivity to unmeasured confounding and the methods that exist either depend on the analysis method used or are on a particular (non-universal) scale, we cannot recommend a specific approach and instead recommend researchers attend carefully to developments in this area and follow the norms of their respective fields.
