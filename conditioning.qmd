# Conditioning: matching and weighting {#sec-conditioning}

In the conditioning step, we perform the matching or weighting on the data, which adjusts the sample in such a way that (ideally) balance is achieved, i.e., the distributions of the covariates are similar between the treated and untreated groups. There are many options one can choose from in the conditioning step, such as which of the propensity score methods is used and how each method is to be customized. Remember that the initial conditioning step does not need to be perfect because it will be respecified in the respecification step (@sec-respecification). In @sec-assessing, we discuss how to evaluate the choices made in the conditioning step.

First, we will describe weighting and then matching. The focus here will be on the choices one can make with these methods and how to make them rather than on the technical detail.

## Weighting {#sec-weighting}

Weighting involves estimating a weight for each unit in the sample such that, in the weighted sample, the covariates are balanced. Effect estimation proceeds by computing the weighted outcome means in each treatment group and computing their contrast or fitting a weighted outcome regression model with the treatment as a predictor. Weighting can be used to target any estimand, but the formulas for how the weights are computed depends on the estimand desired. Balancing weights function similarly to sampling weights; while sampling weights shift a sample to be representative of a national population, balancing weights shift each treatment group to resemble each other and a target population.

The most basic form of weighting is propensity score weighting for the ATE, also known as inverse probability of treatment weighting (IPTW). The weights have the following formula:

$$
w_{ATE, i} = \begin{cases}
  1/p_i, & \text{if } A_i = 1, \\
  1/(1-p_i), & \text{if } A_i = 0
\end{cases}
$$

where $p_i$ is an individual's propensity score and $A_i$ is an individual's treatment value, 1 if treated and 0 if untreated. They are known as "inverse probability" weights because the weights are equal to the inverse of the probability of receiving the treatment actually received. IPTW weights shift the distributions of both the treated and untreated groups to resemble that of the full sample and therefore each other.

Propensity score weighting for the ATT is also known as standardized mortality ratio weighting, and the weights have the following formula:

$$
w_{ATT, i} = p_i \times w_{ATE, i} = \begin{cases}
  1, & \text{if } A_i = 1, \\
  p_i/(1-p_i), & \text{if } A_i = 0
\end{cases}
$$

Weights for the ATT shift the distribution of the untreated units to resemble that of the treated units and leave the treated units untouched.

Propensity score weighting for the ATO is also known as overlap weighting, and the weights have the following formula:

$$
w_{ATO, i} = p_i(1-p_i) \times w_{ATE, i} = \begin{cases}
  1-p_i, & \text{if } A_i = 1, \\
  p_i, & \text{if } A_i = 0
\end{cases}
$$

Overlap weights upweight units most like those in the other treatment group. Though there are other methods to compute weights that target an overlap sample (e.g., the "matching weights" of @liWeightingAnaloguePair2013), overlap weights tend to outperform them and produce a weighted sample with the most precision of any propensity score weights.

A choice that researchers must make when using propensity score weighting is how to estimate the propensity score. This choice affects the properties of the weights (i.e., the balance they induces and the precision in the weighted sample). The most common method is a logistic regression of the treatment on the covariates. Other popular methods involve machine learning methods like generalized boosted modeling (GBM) [@mccaffreyPropensityScoreEstimation2004], Bayesian additive regression trees (BART) [@hillChallengesPropensityScore2011], and Super Learner [@alamShouldPropensityScore2019], though any model that produces predicted class probabilities can be used to estimate propensity scores. Often, versions of these methods incorporate balance optimization into the estimation of the weights; for example, a popular implementation of GBM chooses the value of a tuning parameter as that which minimizes an imbalance statistic [@mccaffreyPropensityScoreEstimation2004]. Logistic regression has a particular benefit when using overlap weights: the covariate means will be exactly balanced between the treatment groups.

Many modern methods skip the step of estimating a propensity score and estimate the weights directly. Examples of this approach include entropy balancing [@hainmuellerEntropyBalancingCausal2012; @zhaoEntropyBalancingDoubly2017], stable balancing weights [@zubizarretaStableWeightsThat2015], and energy balancing [@hulingEnergyBalancingCovariate2022]. This distinction is explored in detail by @chattopadhyayBalancingVsModeling2020. A popular weighting method, covariate balancing propensity score (CBPS) weighting, combines optimization and logistic regression-based propensity score estimation [@imaiCovariateBalancingPropensity2014] (though this does not necessarily confer any benefits over methods that don't estimate a propensity score [@liPropensityScoreAnalysis2021a]). These optimization-based methods often exactly or approximately balance features of the distribution of covariates while retaining precision in the weighted sample, making them highly effective[^conditioning-1].

[^conditioning-1]: A common theme in many areas of statistics is that newer methods perform better than older methods in general. However, those newer methods are often less well studied and opaque or unheard of by applied researchers. Their absence from the applied literature and the popularity of older, more basic methods is not an indication that the basic methods should be preferred; rather, it often reflects the fear or ignorance by researchers and reviewers of newer, better performing methods.

Weighting methods sometimes yield "extreme" weights, i.e., a few weights that take on a much larger value than the others and dominate the analysis, which can make balance worse and reduce precision. This can occur especially when propensity score weighting for the ATE, as small propensity scores in the treated group and large propensity scores in the untreated group cause weights to be large when inverted. One approach to dealing with extreme weights is to trim the weights, which can involve either removing units with large weights or extreme propensity scores or setting the value of their weights to a smaller value (winsorizing). These methods often change the estimand from the ATE, in which case the ATO should be targeting using overlap weights instead.

## Matching {#sec-matching}

Matching involves dropping or reorganizing units into strata such that the remaining sample is balanced [@greiferMatchingMethodsConfounder2021a]. Examples include pair matching, pure subset selection (in which units are dropped from the sample but no pairing occurs), and subclassification, among others. The outputs of a matching method are a set of matching weights and, if pairing or stratification is done, pair or stratum membership for each unit. Matching weights function identically to propensity score weights as described above; indeed, matching can be seen as a restricted form of weighting, where that restriction can sometimes afford benefits in terms of robustness and precision. @stuartMatchingMethodsCausal2010 provides an excellent introduction to matching.

Below, we briefly describe the two major forms of matching, stratification and subset selection (including pair matching).

### Stratification

Stratification simply involves assigning units into strata such that, within strata, the covariates are balanced between treatment groups. The most straightforward method of stratification is exact matching, in which sets of units with identical covariate values are placed into strata based on those values. Any units with no exact matches in the other treatment group are dropped. The remaining sample will be exactly balanced on all included covariates. In practice, continuous variables or categorical variables with many categories make exact matching impossible. One alternative is coarsened exact matching (CEM) [@iacusCausalInferenceBalance2012; @iacusMultivariateMatchingMethods2011], which is simply exact matching on coarsened version of the covariates. The degree of coarsening is controlled by the researcher to strike a balance between discarding units with no matches and ensuring the units within strata are relatively homogeneous in the covariates.

Another alternative is propensity score subclassification [@rosenbaumReducingBiasObservational1984], in which units are placed into strata based on their propensity score values. The number of strata is decided by the researcher; although early literature recommends as few as 5 subclasses, it is always best to try larger numbers of subclass to find the one that yields the highest quality matches, which can sometimes be in the hundreds depending on the sample size [@desaiPropensityscorebasedFineStratification2017].

The result of stratification is a set of stratification weights. These are computed as follows: 1) compute the proportion of units in each stratum that are treated, 2) for each unit, assign to it this proportion in its stratum as a new stratum "propensity score", and 3) apply the propensity score weighting formulas above to the new stratum propensity score using the formula that corresponds to the desired estimand. In this way, stratification is a form of propensity score weighting in which the weights are estimated using a multi-step procedure rather than directly from the propensity scores. This method is known as marginal mean weighting through stratification (MMWS) [@hongMarginalMeanWeighting2010] or fine stratification [@desaiPropensityscorebasedFineStratification2017]. [This blog post](https://ngreifer.github.io/blog/matching-weights/) explains this idea in more detail.

### Subset selection and pair matching

Subset selection involves taking a subset of units from the original sample and dropping the rest, ideally in such a way that the remaining sample is balanced on the covariates. The most common method of subset selection is pair matching, in which treated units are paired with untreated units, and any unpaired units are dropped. There are a number of ways to customize pair matching to improve the balance and precision of the resulting sample:

-   **The distance measure use to compute the closeness between units**. The most common measure is the propensity score difference between each treated and untreated unit. Other distances include the Mahalanobis distance and its robust variant and the scaled Euclidean distance, which are computed from the covariates directly and do not require a propensity score, though a propensity score can be added as an additional covariate in computing them. A powerful optimization-based matching method called genetic matching adjusts elements of the distance measure used in order to optimize the balance of the resulting sample [@diamondGeneticMatchingEstimating2013]. The best distance measure to use will depend on the unique features of the dataset [@ripolloneImplicationsPropensityScore2018], so several should be tried, though genetic matching automates this process.

-   **The number of matches unit receives.** Each treated unit can receive one or more control units as a match, and this number can be chosen by the researcher. For example, one can request 2:1 matching instead of 1:1 matching, which increases the size of the resulting sample but may worsen balance because worse matches are being included. The number of matches each unit receives can be fixed across all units or varied [@mingSubstantialGainsBias2000].

-   **Whether matching is done with or without replacement.** One can choose whether untreated units can be reused as matches for multiple treated units. Matching with replacement often yields better balance and eliminates the effect of who gets matched first on the resulting sample. Matching with replacement can yield imprecision in the effect estimate when the same untreated unit is matched many times; the number of times each untreated unit can be matched can be limited by the researcher. Inference after matching with replacement can be more challenging than when matching without replacement[^conditioning-2].

-   **The order of matches.** When matching without replacement, the order that treated units are matched matters. There are a variety of ways one can specify this order with varying evidence supporting each choice [@rubinMatchingRemoveBias1973; @austinComparison12Algorithms2014], so it is best to try various orders. An alternative is to use optimal pair matching [@hansenOptimalFullMatching2006; @guComparisonMultivariateMatching1993], which optimizes a global distance criterion.

-   **Calipers and exact matching constraints.** A caliper is a limit on how far two units can be before they are disallowed from being matched. Using calipers can improve balance but decrease precision because additional units are discarded (i.e., those without any matches within the caliper) [@austinComparison12Algorithms2014]. It is very common to place a caliper on the propensity score, though it is also possible to place calipers on covariates directly. Though there has been some research into optimal caliper widths [@austinOptimalCaliperWidths2011], the best caliper will depend on the unique features of the dataset, and so many should be tried and evaluated. An exact matching constraint requires that two unit have identical values of the given covariate in order to be allowed to be matched. When calipers or exact matching constraints are used, balance often improves (sometimes dramatically), but discarding treated units changes the estimand to the ATO, which may not be desired [@greiferChoosingEstimandWhen2021; @rosenbaumBiasDueIncomplete1985]. Applying calipers can also make balance worse if good balance has already been achieved without them [@kingWhyPropensityScores2019].

[^conditioning-2]: Although much early research into statistical inference for matching was done for matching with replacement, the inference methods required highly specific uses of matching and complicated estimators of standard errors [@abadieLargeSampleProperties2006; @abadieMatchingEstimatedPropensity2016]. The most applicable methods of inference for matching with replacement rely on simulation evidence and are only approximations [@hillIntervalEstimationTreatment2006].

There are also methods of subset selection without pairing, such as cardinality and profile matching [@zubizarretaMatchingBalancePairing2014; @cohnProfileMatchingGeneralization2022]. These use optimization to find the largest matched sample that satisfies balance constraints set by the user and are starting to see broader use in medical research [e.g., @niknamUsingCardinalityMatching2022; @fortinIndirectCovariateBalance2022].

### Full matching

Full matching is an effective matching method that is somewhat of a cross between pair matching and stratification [@stuartUsingFullMatching2008; @hansenOptimalFullMatching2006]. Every unit in the sample is assigned to a stratum as with stratification methods, but the strata are formed based on the pairwise distances between units. Full matching tends to outperform other matching methods and can be customized in many of the same ways [@austinOptimalFullMatching2015; @austinEstimatingEffectTreatment2017]. Variations of full matching often run much faster than other matching algorithms [@savjeGeneralizedFullMatching2021]. Unlike other pair matching methods, full matching can be used to estimate the ATT, ATC, or ATE. Full matching can also been seen as a alternative to IPTW that can be more robust to misspecification [@austinPerformanceInverseProbability2017].

## Choosing a specification

We explain how to choose among the variety of weighting and matching methods in @sec-assessing. In short, the choice should depend on covariate balance, precision, and respect of the desired estimand. One does not need to choose a method and commit to it; one can instead try many, assess their quality, and move forward with effect estimation using only the best among those compared. However, this search can be shortened by using methods known to perform exceptionally well or that can be specified to respond to a researcher's precise requirements.

Some methods are more popular in certain fields; for example, medical research often uses matching, whereas epidemiological research often uses weighting. In some cases, the popularity of methods in certain fields reflects real substantive demands, but in many cases it simply reflects trends and cultures or the specific methods emphasized in training materials for students. For example, epidemiological training emphasizes the use of weighting over matching [@hernanCausalInferenceWhat2020], even though they often serve the same purpose and perform equally well [@greiferMatchingMethodsConfounder2021a; @kushCovariateBalanceObservational2022].

One key aspect to remember is that the basic or default method is almost never the best method and should not be used just because it is the most familiar or popular in a field. For example, 1:1 pair matching on the propensity score is a popular matching method in medical research, even though it is uniformly outperformed by genetic matching [@diamondGeneticMatchingEstimating2013] and very often performs worse than methods that are no more difficult to use, such as full matching [@austinOptimalFullMatching2015] and cardinality matching [@viscontiHandlingLimitedOverlap2018a]. Similarly, propensity score weighting often performs worse than modern optimization-based methods like entropy balancing [@hainmuellerEntropyBalancingCausal2012].
